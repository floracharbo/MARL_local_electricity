# structure run
type_learning: 'q_learning' # DDPG, q_learning, DQN
trajectory: False # decide on N actions based on N states instead of one at a time
n_repeats: 10  # number of 'learning trajectories' completely repeated from scratch with new data
n_epochs: 20 # 5e1 number of epochs - note we do n_explore explorations at each learning cycle/epoch
n_explore: 2 # number of exploration epochs before updating q
type_env: 'discrete' # action and state space - discrete or continuous
ncpu: 10  # if ran on a server, ncup will be mp.cpu_count()
parallel: False
eval_deterministic: True  # the evaluation actions are deterministic (no eps-greedy)
default_action: 1 # 'baseline' action, i.e. alway import / comsume as much as possible
check_feasibility_with_opt: False  # we attempt to run convex optimisations on each new day of data generated to test feasibility rather than just heuristic checks
offset_reward: False  # offset all rewards by delta_reward
delta_reward: 0.05  # value to offset rewards by
competitive: False # individual agents learn ased in maximising their own rewards - i still plot total rewards though during evaluation -> constrains to decentralised learning
n_end_test: 10  # number of episodes to test at the end of each learning cycle
share_epochs_start_end_eval: 0.8  # share of epochs at the end of which we start evaluating
save_model_interval: 2

# state space
n_other_states: 3  # when using discrete methods (q-learning), how many state intervals
n_grdC_level: 3  # when using discrete methods (q-learning), how many state intervals for the grid prices
n_discrete_actions: 10  # when using discrete methods (q-learning), how many intervals for the actions
state_space: ['grdC']  # default observation available to each agent
# grdC_level, time_step_day, car_tau, store0, grdC
# avail_car_step, loads_clus_step, loads_fact_step
# gen_fact_step, car_fact_step, loads_cons_step, gen_prod_step
# car_cons_step, dT, dT_next, car_cons_prev
# car_dem_agg, gen_fact_prev, car_fact_prev, loads_cons_prev
# gen_prod_prev, car_clus_step, car_clus_prev, loads_clus_prev
# avail_car_prev, loads_fact_prev, day_type, car_cons_step, car_fact_step, bool_flex, store_bool_flex
# flexibility

# deterministic
deterministic: 0  # 0 is indaeterministic, 1 is deterministic, 2 is deterministic noisy
n_init_same_env: 2  # if deterministic
ind_seed_deterministic: 0

# reward
penalty: 10 # if avail_car :: 0 and l_car > store:apply penalty to reward (for training, not evaluation)
apply_penalty: False # if penalty :: 1 and evaluation :: False: step_vals_i['reward'] -: 'learn_prm.penalty

# for q-learning
initialise_q: 'zeros' # zeros or random
instant_feedback: False
control_window_eps: 5
# {'r': 5, 'A' : 5, 'n': 5, 'd' : 2}, # initially if control_eps = 1, what is the length of the control window, when checking if rewards[- window * n_explore] > rewards[-2*window*n_explore:-window*n_explore]
tauMT: 5
    # {'advantage`: 8, 'greedy': 8, 'dgreedy': 1}, #adv10 var[0], # 8/2 mid-term reward time constant as per XU et al 2018 - Reward-Based Exploration
tauLT: 5
    # {'advantage': 8, 'greedy': 8, 'dgreedy': 1}, #adv10 var[0], # 8 /2 long-term reward time constant as per XU et al 2018 - Reward-Based Exploration
T0: 1.e+5
Tend: 1.e-6
distr_learning: 'decentralised' # 'joint', 'central', 'decentralised'
difference_bool: True
opt_bool: True

# lambda âˆˆ [0, 1) is a weighted parameter, which decides the effect of reward difference on the selection of action.
# this is mu in XU et al 2018 - Reward-Based Exploration
lambda: 0.5

q_learning:
        alpha: 0.1 # learning rate e.g. {'opt_all_A' : 0.1, 'greedy_1_reward': 0.01} for q learning
        policy: 'eps-greedy' # 'for DQN and Q learning boltzmann', 'mixed' 'eps-greedy'
        epsilon_decay: 0 # 0 : fixed eps value, 1 - exponential decay, 2 - incremental decay if above baseline + above average
        T_decay: False
        T: 0.01 # {'advantage': 0.1, 'greedy': 0.1, 'dgreedy' : 0.1}, #var[1], #0.08 /  5 in XU et al 2018 - Reward-Based Exploration
        epsilon0: 1
        epsilon_end: 1.e-2
        start_decay: 0
        end_decay: 500
        control_eps: 0 # if 0, no control, if 1 do eps*k if improving else eps*(1/k), if 2 do XU et al. 2018 Reward-Based Exploration
        best_eps_end: {
                0: { 'advantage': 1.e-2, 'dgreedy': 1.e-1, 'greedy': 1.e-1 },
                1: { 'advantage': 1.e-3, 'dgreedy': 5.e-2, 'greedy': 1.e-2 } } #{'opt_1_A': 1e-3, 'greedy_1_reward': 1e-3, 'opt_n': 1e-3, 'opt_reward':1e-2},
        eps: 0.1
        hysteretic: True # if True, Use the default learn_rate as the larger learning rate, and beta as the smaller learning rate. lr = self.learn_rate if td_error > 0 else self.beta
        beta_to_alpha: 0.5
        gamma: 0.7

# DQN
DQN:
        alpha: 0.01 # learning rate
        policy: 'eps-greedy' # 'for DQN and Q learning boltzmann', 'mixed' 'eps-greedy'
        epsilon_decay: 0 # 0 : fixed eps value, 1 - exponential decay, 2 - incremental decay if above baseline + above average
        T_decay: False
        T: 0.01 # {'advantage': 0.1, 'greedy': 0.1, 'dgreedy' : 0.1}, #var[1], #0.08 /  5 in XU et al 2018 - Reward-Based Exploration
        epsilon0: 1
        epsilon_end: 1.e-2
        start_decay: 0
        end_decay: 500
        control_eps: 0 # if 0, no control, if 1 do eps*k if improving else eps*(1/k), if 2 do XU et al. 2018 Reward-Based Exploration
        best_eps_end: {
                0: { 'advantage': 1.e-2, 'dgreedy': 1.e-1, 'greedy': 1.e-1 },
                1: { 'advantage': 1.e-3, 'dgreedy': 5.e-2, 'greedy': 1.e-2 } } #{'opt_1_A': 1e-3, 'greedy_1_reward': 1e-3, 'opt_n': 1e-3, 'opt_reward':1e-2},
        eps: 0.5
        rdn_eps_greedy: False
        rdn_eps_greedy_indiv: False # instead of just adding noise to selected action in DDPG, chose either a random action or the selected action for each item of the list of actions to select at once based on a list of random numbers
        decay_alpha: 0.99 # for DQN
        min_alpha: 1.e-6  # for DQN
        batch_size: 64 # Num of tuples to train on.
        buffer_capacity: 1000 # 100000 Number of "experiences" to store at max
        gamma: 0.85

# DDQN
DDQN:
        epsilon0: 1
        epsilon_end: 1.e-2
        start_decay: 0
        end_decay: 500
        control_eps: 0 # if 0, no control, if 1 do eps*k if improving else eps*(1/k), if 2 do XU et al. 2018 Reward-Based Exploration
        best_eps_end: {
                0: { 'advantage': 1.e-2, 'dgreedy': 1.e-1, 'greedy': 1.e-1 },
                1: { 'advantage': 1.e-3, 'dgreedy': 5.e-2, 'greedy': 1.e-2 } } #{'opt_1_A': 1e-3, 'greedy_1_reward': 1e-3, 'opt_n': 1e-3, 'opt_reward':1e-2},
        eps: 0.5
        rdn_eps_greedy: False
        rdn_eps_greedy_indiv: False # instead of just adding noise to selected action in DDPG, chose either a random action or the selected action for each item of the list of actions to select at once based on a list of random numbers
        batch_size: 64 # Num of tuples to train on.
        buffer_capacity: 1000 # 100000 Number of "experiences" to store at max
        gamma: 0.85


# DDPG
hysteretic_actor: True
std_dev: 0.2 # for noise DDPG
actor_lr: 0.001 # initially  0.001
tau: 0.005 # Used to update target networks
activation: 'relu' # activation function for DDPG; default was 'relu'
init_weight_mult: 1 # intiialisation of network weights multiplier
LSTM: False # replacing intermediate dense layers with LSTM
dim_out_layer12: 256 # dumensionality of the output space of the first two layer of both actor and critic networks
init_weights_zero: True
DDPG:
        critic_lr: 0.002 # initially 0.002,
        rdn_eps_greedy: False
        rdn_eps_greedy_indiv: True # instead of just adding noise to selected action in DDPG, chose either a random action or the selected action for each item of the list of actions to select at once based on a list of random numbers
        decay_alpha: 0.99
        min_alpha: 1.e-6
        hysteretic: True # if True, Use the default learn_rate as the larger learning rate, and beta as the smaller learning rate. lr = self.learn_rate if td_error > 0 else self.beta
        beta_to_alpha: 0.5
        batch_size: 10 # Num of tuples to train on.
        buffer_capacity: 1000 # 100000 Number of "experiences" to store at max
        gamma: 0.85

use_cuda: True # Use gpu by default unless it isn't available
low_action: [0]
high_action: [1]
all_low_actions: [0, 0, -1, -1]
high_actions: [1, 1, 1, 1]
actions_dtype: np.float32
discretize_actions: False
buffer_cpu_only: True
agent_output_type: ~
action_selector: ~

# varied and set to
agent_facmac: mlp
mixer: "qmix"
learner: "facmac_learner"
mac: cqmix_mac
obs_last_action: False # default was True - Include the agent's last action  (one_hot) in the observation
obs_agent_id: True # Include the agent's one_hot id in the observation
rnn_hidden_dim: 5.e+2 # for rnn agent (from 64)
exploration_mode: "gaussian"  # or eps greedy or ornstein_uhlenbeck
ou_stop_episode: 1000 # for cqmix controller - training noise goes to zero after this episode
start_steps: 100 # Number of steps for uniform-random action selection, before running real policy. Helps exploration.
act_noise: 0 # Stddev for Gaussian exploration noise added to policy at training time.
hyper_initialization_nonzeros: 1
#lr: 1.e-4 # Q learning learning rate
buffer_size: 5000
optimizer: adam # D
critic_optimizer: adam
mixing_embed_dim: 64
q_embed_dim: 1
hypernet_layers: 1
hypernet_embed: 64
gated: False
skip_connections: False
optimizer_epsilon: 0.01 # D
grad_norm_clip: 0.5
target_update_mode: 'soft'
target_update_tau: 0.001
recurrent_critic: False
buffer_warmup: 0
agent_return_logits: False
ou_theta: 0.15 # D
ou_sigma: 0.2 # D
ou_noise_scale: 0.3
p_dropout: 0

# to vary
batch_size_run: 1
runner_scope: 'episodic'
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp
evaluation_methods:

facmac:
        # varied
        critic_lr: 1.e-2
        hysteretic: True
        beta_to_alpha: 0.1
        # to vary
        batch_size: 10 # Num of tuples to train on.
        gamma: 0.85
        epsilon: 0.5
        epsilon_decay: False
        epsilon_end: 1.e-2
        epsilon0: 0.8
        control_eps: 0
        lr_decay: False
        lr0: 1.e-2
        lr_end: 1.e-4
        critic_lr0: 5.e-3
        critic_lr_end: 5.e-5
        lr: 5.e-1

aggregate_actions: False

type_learn_to_space:
        q_learning: 'discrete'
        DQN: 'discrete'
        DDQN: 'discrete'
        facmac: 'continuous'
        DDPG: 'continuous'
action_labels_aggregate: ['Action variable [-]']
action_labels_disaggregate: [
        'Flexible consumption action [-]',
        'Flexible heating action [-]',
        'Flexible EV charge action [-]',
        'Flexible battery reactive power action'
]

no_flex_action: 'random' # 'target' 'random' 'None', 'one'
n_start_opt_explo: 0
supervised_loss: False
expert_margin: 0.8
supervised_loss_weight: 0
nn_learned: True
normalise_states: True
nn_type: 'linear' # 'linear' 'cnn' 'lstm'
nn_type_critic: 'linear'
num_layers_lstm: 1
cnn_out_channels: 5
cnn_kernel_size: 3
data_parallel: False # if True, use nn.DataParallel
n_cnn_layers: 1
n_cnn_layers_critic: 1
n_hidden_layers: 1
n_hidden_layers_critic: 1
num_layers_rnn: 1
grdC_n: 0
flexibility_states: ['bool_flex', 'store_bool_flex', 'flexibility']
n_epochs_supervised_loss: 0
homes_exec_per_home_train: ~
device: cpu
q_noise_0: 1.e-8  # for initialise_q = bias_towards_0 / random
mixer_relu: True
normalisation_reward: 1
initialise_positive_weights_hyper_b_1_bias: False
pruning_rate: 0.5  # Percentage of weights to prune
