# structure run
type_learning: 'q_learning' # DDPG, q_learning, DQN
trajectory: False # decide on N actions based on N states instead of one at a time
n_repeats: 10
n_epochs: 50 # 5e1 number of epochs - note we do n_explore explorations at each learning cycle/epoch
n_explore: 2 # number of exploration epochs before updating q
type_env: 'discrete' # action and state space - discrete or continuous
ncpu: 10  # if ran on a server, ncup will be mp.cpu_count()
parallel: False
eval_deterministic: True
default_action: 1 # 'baseline' action, i.e. alway import / comsume as much as possible
check_feasibility_with_opt: False
offset_reward: False
delta_reward: 0.05
competitive: False # individual agents learn ased in maximising their own rewards - i still plot total rewards though during evaluation -> constrains to decentralised learning
server: False
n_end_test: 10
share_epochs_start_end_eval: 0.8

# state space
n_other_states: 3
n_grdC_level: 3
n_discrete_actions: 10
state_space: ['grdC_level']
# grdC_level, hour, EV_tau, store0, grdC
# avail_EV_step, loads_clus_step, loads_fact_step
# gen_fact_step, bat_fact_step, loads_cons_step, gen_prod_step
# bat_cons_step, dT, dT_next, bat_cons_prev
# bat_dem_agg, gen_fact_prev, bat_fact_prev, loads_cons_prev
# gen_prod_prev, bat_clus_step, bat_clus_prev, loads_clus_prev
# avail_EV_prev, loads_fact_prev, day_type, EV_cons_step, EV_fact_step

# deterministic
deterministic: 0  # 0 is indaeterministic, 1 is deterministic, 2 is deterministic noisy
n_init_same_env: 2  # if deterministic
ind_seed_deterministic: 0

# plotting
plotting_mu: False  # used for plotting mu position on graph
plotting_batch: True

# reward
penalty: 10 # if avail_EV :: 0 and l_EV > store:apply penalty to reward (for training, not evaluation)
apply_penalty: False # if penalty :: 1 and evaluation :: False: step_vals_i['reward'] -: 'learn_prm.penalty

# for q-learning
initialise_q: 'zeros' # zeros or random
instant_feedback: False
control_window_eps: 5
# {'r': 5, 'A' : 5, 'n': 5, 'd' : 2}, # initially if control_eps = 1, what is the length of the control window, when checking if rewards[- window * n_explore] > rewards[-2*window*n_explore:-window*n_explore]
tauMT: 5
    # {'advantage`: 8, 'greedy': 8, 'dgreedy': 1}, #adv10 var[0], # 8/2 mid-term reward time constant as per XU et al 2018 - Reward-Based Exploration
tauLT: 5
    # {'advantage': 8, 'greedy': 8, 'dgreedy': 1}, #adv10 var[0], # 8 /2 long-term reward time constant as per XU et al 2018 - Reward-Based Exploration
T0: 1.e+5
Tend: 1.e-6
distr_learning: 'decentralised' # 'joint', 'central', 'decentralised'
difference_bool: True
opt_bool: True

# lambda âˆˆ [0, 1) is a weighted parameter, which decides the effect of reward difference on the selection of action.
# this is mu in XU et al 2018 - Reward-Based Exploration
lambda: 0.5

q_learning:
        alpha: 0.01 # learning rate e.g. {'opt_all_A' : 0.1, 'greedy_1_reward': 0.01} for q learning
        policy: 'eps-greedy' # 'for DQN and Q learning boltzmann', 'mixed' 'eps-greedy'
        epsilon_decay: 0 # 0 : fixed eps value, 1 - exponential decay, 2 - incremental decay if above baseline + above average
        T_decay: False
        T: 0.01 # {'advantage': 0.1, 'greedy': 0.1, 'dgreedy' : 0.1}, #var[1], #0.08 /  5 in XU et al 2018 - Reward-Based Exploration
        epsilon0: 1
        epsilon_end: 1.e-2
        start_decay: 0
        end_decay: 500
        control_eps: 0 # if 0, no control, if 1 do eps*k if improving else eps*(1/k), if 2 do XU et al. 2018 Reward-Based Exploration
        best_eps_end: {
                0: { 'advantage': 1.e-2, 'dgreedy': 1.e-1, 'greedy': 1.e-1 },
                1: { 'advantage': 1.e-3, 'dgreedy': 5.e-2, 'greedy': 1.e-2 } } #{'opt_1_A': 1e-3, 'greedy_1_reward': 1e-3, 'opt_n': 1e-3, 'opt_reward':1e-2},
        eps: 0.5
        hysteretic: True # if True, Use the default learn_rate as the larger learning rate, and beta as the smaller learning rate. lr = self.learn_rate if td_error > 0 else self.beta
        beta_to_alpha: 0.5
        gamma: 0.85

# DQN
DQN:
        alpha: 0.01 # learning rate
        policy: 'eps-greedy' # 'for DQN and Q learning boltzmann', 'mixed' 'eps-greedy'
        epsilon_decay: 0 # 0 : fixed eps value, 1 - exponential decay, 2 - incremental decay if above baseline + above average
        T_decay: False
        T: 0.01 # {'advantage': 0.1, 'greedy': 0.1, 'dgreedy' : 0.1}, #var[1], #0.08 /  5 in XU et al 2018 - Reward-Based Exploration
        epsilon0: 1
        epsilon_end: 1.e-2
        start_decay: 0
        end_decay: 500
        control_eps: 0 # if 0, no control, if 1 do eps*k if improving else eps*(1/k), if 2 do XU et al. 2018 Reward-Based Exploration
        best_eps_end: {
                0: { 'advantage': 1.e-2, 'dgreedy': 1.e-1, 'greedy': 1.e-1 },
                1: { 'advantage': 1.e-3, 'dgreedy': 5.e-2, 'greedy': 1.e-2 } } #{'opt_1_A': 1e-3, 'greedy_1_reward': 1e-3, 'opt_n': 1e-3, 'opt_reward':1e-2},
        eps: 0.5
        rdn_eps_greedy: False
        rdn_eps_greedy_indiv: False # instead of just adding noise to selected action in DDPG, chose either a random action or the selected action for each item of the list of actions to select at once based on a list of random numbers
        decay_alpha: 0.99 # for DQN
        min_alpha: 1.e-6  # for DQN
        batch_size: 64 # Num of tuples to train on.
        buffer_capacity: 1000 # 100000 Number of "experiences" to store at max
        gamma: 0.85

# DDQN
DDQN:
        epsilon0: 1
        epsilon_end: 1.e-2
        start_decay: 0
        end_decay: 500
        control_eps: 0 # if 0, no control, if 1 do eps*k if improving else eps*(1/k), if 2 do XU et al. 2018 Reward-Based Exploration
        best_eps_end: {
                0: { 'advantage': 1.e-2, 'dgreedy': 1.e-1, 'greedy': 1.e-1 },
                1: { 'advantage': 1.e-3, 'dgreedy': 5.e-2, 'greedy': 1.e-2 } } #{'opt_1_A': 1e-3, 'greedy_1_reward': 1e-3, 'opt_n': 1e-3, 'opt_reward':1e-2},
        eps: 0.5
        rdn_eps_greedy: False
        rdn_eps_greedy_indiv: False # instead of just adding noise to selected action in DDPG, chose either a random action or the selected action for each item of the list of actions to select at once based on a list of random numbers
        batch_size: 64 # Num of tuples to train on.
        buffer_capacity: 1000 # 100000 Number of "experiences" to store at max
        gamma: 0.85


# DDPG
hysteretic_actor: True
std_dev: 0.2 # for noise DDPG
actor_lr: 0.001 # initially  0.001
tau: 0.005 # Used to update target networks
activation: 'relu' # activation function for DDPG; default was 'relu'
init_weight_mult: 1 # intiialisation of network weights multiplier
LSTM: False # replacing intermediate dense layers with LSTM
dim_out_layer12: 256 # dumensionality of the output space of the first two layer of both actor and critic networks
DDPG:
        critic_lr: 0.002 # initially 0.002,
        rdn_eps_greedy: False
        rdn_eps_greedy_indiv: False # instead of just adding noise to selected action in DDPG, chose either a random action or the selected action for each item of the list of actions to select at once based on a list of random numbers
        decay_alpha: 0.99
        min_alpha: 1.e-6
        hysteretic: True # if True, Use the default learn_rate as the larger learning rate, and beta as the smaller learning rate. lr = self.learn_rate if td_error > 0 else self.beta
        beta_to_alpha: 0.5
        batch_size: 10 # Num of tuples to train on.
        buffer_capacity: 1000 # 100000 Number of "experiences" to store at max
        gamma: 0.85

#DDQN DDPG DQN

# facmac
# fixed
#dim_actions: 1
use_cuda: True # Use gpu by default unless it isn't available
low_action: [0]
high_action: [1]
low_actions: [0, 0, -1]
high_actions: [1, 1, 1]
actions_dtype: np.float32
discretize_actions: False
episode_limit: 24
buffer_cpu_only: True
agent_output_type: ~
action_selector: ~

# varied and set to
agent_facmac: mlp
mixer: "qmix"
learner: "facmac_learner"
mac: cqmix_mac
obs_last_action: False # default was True - Include the agent's last action  (one_hot) in the observation
obs_agent_id: True # Include the agent's one_hot id in the observation
rnn_hidden_dim: 1.e+4 # for rnn agent (from 64)
n_hidden_layers: 1
exploration_mode: "gaussian"
ou_stop_episode: 100 # for cqmix controller - training noise goes to zero after this episode
start_steps: 0 # Number of steps for uniform-random action selection, before running real policy. Helps exploration.
act_noise: 0.1 # Stddev for Gaussian exploration noise added to policy at training time.
hyper_initialization_nonzeros: 0
lr: 1.e-6
buffer_size: 5000
optimizer: adam # D
mixing_embed_dim: 64
q_embed_dim: 1
hypernet_layers: 1
hypernet_embed: 64
gated: False
skip_connections: False
optimizer_epsilon: 0.01 # D
grad_norm_clip: 0.5
target_update_mode: 'soft'
target_update_tau: 0.001
recurrent_critic: False
buffer_warmup: 0
agent_return_logits: False
ou_theta: 0.15 # D
ou_sigma: 0.2 # D
ou_noise_scale: 0.3

# to vary
batch_size_run: 1
runner_scope: 'episodic'
optim_alpha: 0.99 # RMSProp alpha
optim_eps: 0.00001 # RMSProp

facmac:
        # varied
        critic_lr: 1.e-6
        # to vary
        batch_size: 10 # Num of tuples to train on.
        gamma: 0.85

aggregate_actions: True

# i think this is not used
normalise_actions: False

#i think this has to be episodic